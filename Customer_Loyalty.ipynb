{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ae519",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These Codes are Written by Mehdi Touyserkani (ir-bestpro)\n",
    "## Email : ir_bestpro@yahoo.com\n",
    "## Website : https://www.ir-bestpro.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80e152-f254-4e85-af17-f4cad43e10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput,ProcessingOutput\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import (LocalPipelineSession , PipelineSession)\n",
    "from sagemaker.workflow.steps import (ProcessingStep,TrainingStep)\n",
    "from sagemaker.workflow.parameters import (ParameterInteger,ParameterString,ParameterFloat)\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ccff1-891e-49a1-985d-d49f620b5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________________Global Variables_____________________________________________\n",
    "\n",
    "session = PipelineSession() #LocalPipelineSession() \n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker.session.Session().default_bucket() # Get default bucket of user's aws space\n",
    "\n",
    "bucket_name = 'customers-dataset'\n",
    "local_path = './dataset/Customers.csv' # get local path of directories\n",
    "\n",
    "#__Get access to Dataset and Download it to local Notepad path_____\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket_name).download_file(\"Customers.csv\", local_path)\n",
    "\n",
    "#__________Uplaod dataset to default s3 bucket_____________________\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/loyalty\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path = local_path,\n",
    "    desired_s3_uri = base_uri,\n",
    ")\n",
    "\n",
    "#__create workflow parameters for pipeline object instance_______\n",
    "\n",
    "input_dataset = ParameterString( # set pipeline input data\n",
    "    name = \"InputData\",\n",
    "    default_value = input_data_uri,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger( # set number of processors\n",
    "    name = \"ProcessingInstanceCount\",\n",
    "    default_value = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945f286-f26f-4dc0-a004-a971b51bb413",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def Entropy(X):\n",
    "    Entropies = np.ones((1,X.shape[1]))\n",
    "    Entropies = list([entropy(np.unique(X[:,i], return_counts=True)[1],base=10) for i in range(0,X.shape[1])])\n",
    "    Entropies = sorted(enumerate(Entropies),key = lambda x: x[1])\n",
    "    selected_features = list(map(lambda x: x[0], [Entropies[i] for i in range(0,math.floor(len(Entropies) * .5))]))\n",
    "    return (selected_features)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\" # address to container image folder for saving the input and output data\n",
    "\n",
    "    print('Starting Preprocessing Step')\n",
    "    \n",
    "    #_________Reading the input csv file at first______________\n",
    "    \n",
    "    Dataset = pd.read_csv(f\"{base_dir}/input/Customers.csv\" , header=0) # address to base data directory in container and read the data files\n",
    "    X = Dataset.iloc[:,:-1]\n",
    "    Gender = {\"Female\" : 1 , \"Male\" : 0}\n",
    "    X['Gender'] = X['Gender'].map(Gender)\n",
    "    Y = Dataset.iloc[:,-1].to_numpy()\n",
    "\n",
    "    #_______________Data Smooting________________________________\n",
    "\n",
    "    nan_df = X.isna().any(axis=1) # get access to nan values data\n",
    "    nan_df = list(map(lambda element: element[0] , filter(lambda idx: idx[1]== True , enumerate(nan_df)))) # extract nan values\n",
    "\n",
    "    for data in nan_df:\n",
    "        nan_row = X.iloc[data , :].isna().any(axis=2) # extract coloumns\n",
    "        nan_cols = list(map(lambda element: element[0] , filter(lambda idx: idx[1]== True , enumerate(nan_row)))) # extract nan values\n",
    "        map(lambda pointer : X.iloc[data , pointer].interpolate() , nan_cols) # Estimating Nan Values Based on it's Neighbours (Mean and interpolation)\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    \n",
    "    #_________________Noise Detection and removal___________________\n",
    "\n",
    "    dist = (np.average(list(map(lambda x: np.linalg.norm(x - X) , X))) / X.shape[0]) / (0.05 * X.shape[1])\n",
    "    clustering = DBSCAN(eps=dist, min_samples=3).fit(X) # Output -1 Equals to Noise\n",
    "    noise_indexes = np.array(list(map( lambda y: y[0] , filter(lambda x : x[1] == -1 , enumerate(clustering.labels_))))) # Select Noise Indexes\n",
    "    clean_indexes = np.array(list(filter(lambda x: noise_indexes.__contains__(x) == False ,range(0,X.shape[0])))) # Selecting Clean Indexes\n",
    "\n",
    "    X = X[clean_indexes , :] # Remove Noise Indexes from X\n",
    "    Y = Y[clean_indexes] # Remove Noise Indexes from Y\n",
    "    Entropy_features = Entropy(X) # The best Features Base on sample Entropy function\n",
    "\n",
    "    #__________Save Preprocessed data in separate files_____________\n",
    "\n",
    "    X = X[: , Entropy_features] # feature selection\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "    pd.DataFrame(np.column_stack((X_train , y_train))).to_csv(f\"{base_dir}/output/Train/Train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(np.column_stack((X_test , y_test))).to_csv(f\"{base_dir}/output/Test/Test.csv\", header=False, index=False)\n",
    "\n",
    "print('The first step is executed now!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07857d66-0b7b-42ac-a3e8-4d6b21ab9b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = [ProcessingInput  (source = input_dataset   , destination = '/opt/ml/processing/input')] # copy local data to sklearn container\n",
    "outputs = [ProcessingOutput (output_name = 'Train' , source = '/opt/ml/processing/output/Train') , \n",
    "           ProcessingOutput (output_name = 'Test'  , source = '/opt/ml/processing/output/Test')] # copy preprocessed data to sklearn container\n",
    "\n",
    "processing_job_instance = SKLearnProcessor(framework_version ='1.0-1',\n",
    "                                           role = role,\n",
    "                                           sagemaker_session = session,\n",
    "                                           base_job_name = \"customer-loyalty\",\n",
    "                                           instance_type = 'ml.t3.medium',\n",
    "                                           instance_count = 1)\n",
    "\n",
    "preprocessing_step = ProcessingStep(name = \"PreProcessingStep\",\n",
    "                                    step_args = processing_job_instance.run(inputs = inputs, outputs = outputs,code = \"preprocessing.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ea802-75fd-4f03-a3b6-72d31c37eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model/train-model.py\n",
    "\n",
    "#_________Import Libraries_______________________\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pickle\n",
    "\n",
    "class Dense_Network(nn.Module):\n",
    "\n",
    "    def __init__(self , features_length , number_of_clesses):\n",
    "        super(Dense_Network , self).__init__()\n",
    "        self.FC1 = nn.Sequential(nn.Linear(features_length,256) , nn.BatchNorm1d(256) , nn.ReLU())\n",
    "        self.FC2 = nn.Sequential(nn.Linear(256,512) , nn.BatchNorm1d(512) ,  nn.ReLU())\n",
    "        self.FC3 = nn.Sequential(nn.Linear(512,1024) , nn.BatchNorm1d(1024) ,  nn.ReLU())\n",
    "        self.FC4 = nn.Sequential(nn.Linear(1024,number_of_clesses))\n",
    "\n",
    "    def forward(self , x):\n",
    "        out = self.FC1(x)\n",
    "        out = self.FC2(out)\n",
    "        out = self.FC3(out)\n",
    "        out = self.FC4(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #______________setting the model hyperparameters_______________\n",
    "\n",
    "    LEARNING_RATE = 0.01\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    base_dir = \"/opt/ml/input/data\" # base directory of local container input data (Training step container)\n",
    "\n",
    "    train_Data = pd.read_csv(f\"{base_dir}/train/Train.csv\").to_numpy()\n",
    "    X_train = train_Data[:,:-1] # get train csv file from processing step output\n",
    "    y_train = train_Data[:,-1]\n",
    "\n",
    "    #______________Create Dense_Network Instances_____________________\n",
    "\n",
    "    model = Dense_Network( X_train.shape[1] , 1) # create Dense_Network Instance\n",
    "    optimizer = torch.optim.Adam(model.parameters() , LEARNING_RATE) # using adam weight Optimizer\n",
    "    Loss = nn.BCEWithLogitsLoss() # using binrary cross entropy loss function\n",
    "\n",
    "    #_______________Train Phase__________________\n",
    "\n",
    "    for epoch in range(EPOCHS): # running the model in N epochs\n",
    "        temp_loss = 0 # temp loss in each epoch\n",
    "        steps = 0 # counting number of batches\n",
    "        temp_acc = 0\n",
    "        for batch in range(0 , len(X_train) , BATCH_SIZE): # reading all coefficients in serveral batches (80% For Train)\n",
    "\n",
    "            X = torch.from_numpy(X_train[batch : (batch + BATCH_SIZE) , :]).float() # reading current coefficients vectors\n",
    "            targets = torch.from_numpy(y_train[batch : (batch + BATCH_SIZE)]).float() # reading datalabels\n",
    "            output = model(X).squeeze(1) # calling the model and get generated outputs\n",
    "            loss = Loss(output, targets) # calc loss function\n",
    "            temp_loss += loss.item() # accumulative sum of loss values\n",
    "            steps += 1\n",
    "\n",
    "            model.zero_grad() # don't save gradient history\n",
    "            loss.backward() # backPropagation process\n",
    "            optimizer.step() # update Weights\n",
    "\n",
    "        print('Train Phase - Epoch # ' , str(epoch + 1) , ', Loss : ' , str(temp_loss / steps))\n",
    "\n",
    "\n",
    "    #____________Saving the model and upload to s3 default bucket___________________\n",
    "\n",
    "    local_model_path = '/opt/ml/model' \n",
    "    pickle.dump(model, open(f\"{local_model_path}/model.sav\", 'wb')) # save model to a separate file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508ef6c-8760-4b46-b6ce-b2fe703c184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________________create training step____________________________________\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "model_estimator = PyTorch('./model/train-model.py', # create estimator instance object\n",
    "                            instance_type ='ml.m4.xlarge',\n",
    "                            instance_count = 1,\n",
    "                            framework_version ='1.8.0',\n",
    "                            py_version = 'py3',\n",
    "                            sagemaker_session = session,\n",
    "                            role = role,\n",
    "                            base_job_name = \"customer-loyalty\")\n",
    "\n",
    "\n",
    "train_step_args = model_estimator.fit({'train': TrainingInput( # run pytorch model\n",
    "                                          s3_data = preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"Train\"].S3Output.S3Uri,\n",
    "                                          content_type=\"text/csv\")})\n",
    "\n",
    "training_step = TrainingStep( # create trining step\n",
    "    name = \"TrainginStep\",\n",
    "    step_args = train_step_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35626755-5b41-41e8-9f4c-e5d63274e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model/evaluation.py\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pickle\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Dense_Network(nn.Module):\n",
    "\n",
    "    def __init__(self , features_length , number_of_clesses):\n",
    "        super(Dense_Network , self).__init__()\n",
    "        self.FC1 = nn.Sequential(nn.Linear(features_length,256) , nn.BatchNorm1d(256) , nn.ReLU())\n",
    "        self.FC2 = nn.Sequential(nn.Linear(256,512) , nn.BatchNorm1d(512) ,  nn.ReLU())\n",
    "        self.FC3 = nn.Sequential(nn.Linear(512,1024) , nn.BatchNorm1d(1024) ,  nn.ReLU())\n",
    "        self.FC4 = nn.Sequential(nn.Linear(1024,number_of_clesses))\n",
    "\n",
    "    def forward(self , x):\n",
    "        out = self.FC1(x)\n",
    "        out = self.FC2(out)\n",
    "        out = self.FC3(out)\n",
    "        out = self.FC4(out)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #___________Get model from s3 bucket_____________________\n",
    "\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\" # local model path in container\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    model = pickle.load(open(\"model.sav\", \"rb\"))\n",
    "    #model = pickle.load(open(f\"{model_path}/model.sav\", \"rb\"))\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test\" # local test data path in container\n",
    "    test_Data = pd.read_csv(f\"{test_path}/Test.csv\").to_numpy()\n",
    "    X_test = test_Data[:,:-1] # get test csv file from processing step output\n",
    "    y_test = test_Data[:,-1]\n",
    "    \n",
    "    X_test = torch.from_numpy(X_test).float() # convert to Tensor object\n",
    "    y_test = torch.from_numpy(y_test).float() # convert to Tensor object\n",
    "\n",
    "    #________________Test Phase______________________________\n",
    "\n",
    "    acc = 0 # accuracy value\n",
    "    #with torch.no_grad(): # stop weight updating\n",
    "    outputs = model(X_test).squeeze(1) # test with 20% of data\n",
    "    predicted = (outputs.data > 0.5).float() # predicted labels\n",
    "    del outputs # free Ram Space\n",
    "    true = (predicted == y_test).sum().item() # correct answers\n",
    "    acc = (true / len(y_test))\n",
    "    print('The Model Accuracy Is : ' , str(100 * (true / len(y_test))) + '%') # Pprint the final accuracy\n",
    "\n",
    "    report_dict = {\n",
    "        \"Model Accuracy\": {\n",
    "            \"acc\": {\"value\": acc},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    #____________save acc value to json file__________________\n",
    "    \n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51cfba-bb74-431e-a9ef-bc706fca8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#________________Create evaluation processing Step_________________________________________\n",
    "\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "\n",
    "eval_inputs  = [ProcessingInput  (source = training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                                  destination = '/opt/ml/processing/model'),\n",
    "                ProcessingInput  (source = preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"Test\"].S3Output.S3Uri,\n",
    "                                  destination = '/opt/ml/processing/test')] # copy local data to sklearn container\n",
    "\n",
    "eval_outputs = [ProcessingOutput (output_name = 'Evaluation' , source = '/opt/ml/processing/evaluation')] # local container output directory\n",
    "\n",
    "\n",
    "evaluation_job_instance = PyTorchProcessor( # run pytorch codes in processing step (evaluation step)\n",
    "                                            framework_version ='1.8',\n",
    "                                            role = role,\n",
    "                                            instance_type ='ml.t3.medium',\n",
    "                                            instance_count =1,\n",
    "                                            sagemaker_session = session,\n",
    "                                            base_job_name ='customer-loyalty')\n",
    "\n",
    "evaluation_step = ProcessingStep(name = \"EvaluationStep\",\n",
    "                                 step_args = evaluation_job_instance.run(inputs = eval_inputs, outputs = eval_outputs, code = \"./model/evaluation.py\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee98e82-9265-4c3e-98a2-105f7ea489d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#______________create pipeline and set parameters_____________________________\n",
    "\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True) # set job instance name for each step\n",
    "\n",
    "pipeline = Pipeline( # create pipeline instance object\n",
    "    name = \"customer-loyalty-pipeline\",\n",
    "    steps = [preprocessing_step , training_step , evaluation_step],\n",
    "    parameters = [processing_instance_count,input_dataset],\n",
    "    sagemaker_session = session,\n",
    "    pipeline_definition_config = definition_config\n",
    ")\n",
    "\n",
    "pipeline.create(\n",
    "    role_arn=role,\n",
    "    description=\"customer-loyalty-pipeline\"\n",
    ")\n",
    "\n",
    "execution = pipeline.start() # start the pipeline\n",
    "execution.list_steps() # show the list of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007e783-4107-43b4-9daf-23cdb8560694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = boto3.client('sagemaker')\n",
    "#response = client.delete_pipeline(PipelineName='customer-loyalty-pipeline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
